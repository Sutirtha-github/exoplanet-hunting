{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"exoTrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "      <th>FLUX.3197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>93.85</td>\n",
       "      <td>83.81</td>\n",
       "      <td>20.10</td>\n",
       "      <td>-26.98</td>\n",
       "      <td>-39.56</td>\n",
       "      <td>-124.71</td>\n",
       "      <td>-135.18</td>\n",
       "      <td>-96.27</td>\n",
       "      <td>-79.89</td>\n",
       "      <td>...</td>\n",
       "      <td>-78.07</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>25.13</td>\n",
       "      <td>48.57</td>\n",
       "      <td>92.54</td>\n",
       "      <td>39.32</td>\n",
       "      <td>61.42</td>\n",
       "      <td>5.08</td>\n",
       "      <td>-39.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-38.88</td>\n",
       "      <td>-33.83</td>\n",
       "      <td>-58.54</td>\n",
       "      <td>-40.09</td>\n",
       "      <td>-79.31</td>\n",
       "      <td>-72.81</td>\n",
       "      <td>-86.55</td>\n",
       "      <td>-85.33</td>\n",
       "      <td>-83.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-24.89</td>\n",
       "      <td>-4.86</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-11.70</td>\n",
       "      <td>6.46</td>\n",
       "      <td>16.00</td>\n",
       "      <td>19.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>532.64</td>\n",
       "      <td>535.92</td>\n",
       "      <td>513.73</td>\n",
       "      <td>496.92</td>\n",
       "      <td>456.45</td>\n",
       "      <td>466.00</td>\n",
       "      <td>464.50</td>\n",
       "      <td>486.39</td>\n",
       "      <td>436.56</td>\n",
       "      <td>...</td>\n",
       "      <td>-71.69</td>\n",
       "      <td>13.31</td>\n",
       "      <td>13.31</td>\n",
       "      <td>-29.89</td>\n",
       "      <td>-20.88</td>\n",
       "      <td>5.06</td>\n",
       "      <td>-11.80</td>\n",
       "      <td>-28.91</td>\n",
       "      <td>-70.02</td>\n",
       "      <td>-96.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>326.52</td>\n",
       "      <td>347.39</td>\n",
       "      <td>302.35</td>\n",
       "      <td>298.13</td>\n",
       "      <td>317.74</td>\n",
       "      <td>312.70</td>\n",
       "      <td>322.33</td>\n",
       "      <td>311.31</td>\n",
       "      <td>312.42</td>\n",
       "      <td>...</td>\n",
       "      <td>5.71</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>30.05</td>\n",
       "      <td>20.03</td>\n",
       "      <td>-12.67</td>\n",
       "      <td>-8.77</td>\n",
       "      <td>-17.31</td>\n",
       "      <td>-17.35</td>\n",
       "      <td>13.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1107.21</td>\n",
       "      <td>-1112.59</td>\n",
       "      <td>-1118.95</td>\n",
       "      <td>-1095.10</td>\n",
       "      <td>-1057.55</td>\n",
       "      <td>-1034.48</td>\n",
       "      <td>-998.34</td>\n",
       "      <td>-1022.71</td>\n",
       "      <td>-989.57</td>\n",
       "      <td>...</td>\n",
       "      <td>-594.37</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-357.24</td>\n",
       "      <td>-443.76</td>\n",
       "      <td>-438.54</td>\n",
       "      <td>-399.71</td>\n",
       "      <td>-384.65</td>\n",
       "      <td>-411.79</td>\n",
       "      <td>-510.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL   FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6  FLUX.7  \\\n",
       "0      2    93.85    83.81    20.10   -26.98   -39.56  -124.71 -135.18   \n",
       "1      2   -38.88   -33.83   -58.54   -40.09   -79.31   -72.81  -86.55   \n",
       "2      2   532.64   535.92   513.73   496.92   456.45   466.00  464.50   \n",
       "3      2   326.52   347.39   302.35   298.13   317.74   312.70  322.33   \n",
       "4      2 -1107.21 -1112.59 -1118.95 -1095.10 -1057.55 -1034.48 -998.34   \n",
       "\n",
       "    FLUX.8  FLUX.9  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
       "0   -96.27  -79.89  ...     -78.07    -102.15    -102.15      25.13   \n",
       "1   -85.33  -83.97  ...      -3.28     -32.21     -32.21     -24.89   \n",
       "2   486.39  436.56  ...     -71.69      13.31      13.31     -29.89   \n",
       "3   311.31  312.42  ...       5.71      -3.73      -3.73      30.05   \n",
       "4 -1022.71 -989.57  ...    -594.37    -401.66    -401.66    -357.24   \n",
       "\n",
       "   FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
       "0      48.57      92.54      39.32      61.42       5.08     -39.54  \n",
       "1      -4.86       0.76     -11.70       6.46      16.00      19.93  \n",
       "2     -20.88       5.06     -11.80     -28.91     -70.02     -96.67  \n",
       "3      20.03     -12.67      -8.77     -17.31     -17.35      13.98  \n",
       "4    -443.76    -438.54    -399.71    -384.65    -411.79    -510.54  \n",
       "\n",
       "[5 rows x 3198 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 3198)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=data[['LABEL']]\n",
    "x_train=data[data.columns[1:3197]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>FLUX.10</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3187</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93.85</td>\n",
       "      <td>83.81</td>\n",
       "      <td>20.10</td>\n",
       "      <td>-26.98</td>\n",
       "      <td>-39.56</td>\n",
       "      <td>-124.71</td>\n",
       "      <td>-135.18</td>\n",
       "      <td>-96.27</td>\n",
       "      <td>-79.89</td>\n",
       "      <td>-160.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.51</td>\n",
       "      <td>-78.07</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>25.13</td>\n",
       "      <td>48.57</td>\n",
       "      <td>92.54</td>\n",
       "      <td>39.32</td>\n",
       "      <td>61.42</td>\n",
       "      <td>5.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-38.88</td>\n",
       "      <td>-33.83</td>\n",
       "      <td>-58.54</td>\n",
       "      <td>-40.09</td>\n",
       "      <td>-79.31</td>\n",
       "      <td>-72.81</td>\n",
       "      <td>-86.55</td>\n",
       "      <td>-85.33</td>\n",
       "      <td>-83.97</td>\n",
       "      <td>-73.38</td>\n",
       "      <td>...</td>\n",
       "      <td>12.53</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-24.89</td>\n",
       "      <td>-4.86</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-11.70</td>\n",
       "      <td>6.46</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>532.64</td>\n",
       "      <td>535.92</td>\n",
       "      <td>513.73</td>\n",
       "      <td>496.92</td>\n",
       "      <td>456.45</td>\n",
       "      <td>466.00</td>\n",
       "      <td>464.50</td>\n",
       "      <td>486.39</td>\n",
       "      <td>436.56</td>\n",
       "      <td>484.39</td>\n",
       "      <td>...</td>\n",
       "      <td>-83.83</td>\n",
       "      <td>-71.69</td>\n",
       "      <td>13.31</td>\n",
       "      <td>13.31</td>\n",
       "      <td>-29.89</td>\n",
       "      <td>-20.88</td>\n",
       "      <td>5.06</td>\n",
       "      <td>-11.80</td>\n",
       "      <td>-28.91</td>\n",
       "      <td>-70.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>326.52</td>\n",
       "      <td>347.39</td>\n",
       "      <td>302.35</td>\n",
       "      <td>298.13</td>\n",
       "      <td>317.74</td>\n",
       "      <td>312.70</td>\n",
       "      <td>322.33</td>\n",
       "      <td>311.31</td>\n",
       "      <td>312.42</td>\n",
       "      <td>323.33</td>\n",
       "      <td>...</td>\n",
       "      <td>7.42</td>\n",
       "      <td>5.71</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>30.05</td>\n",
       "      <td>20.03</td>\n",
       "      <td>-12.67</td>\n",
       "      <td>-8.77</td>\n",
       "      <td>-17.31</td>\n",
       "      <td>-17.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1107.21</td>\n",
       "      <td>-1112.59</td>\n",
       "      <td>-1118.95</td>\n",
       "      <td>-1095.10</td>\n",
       "      <td>-1057.55</td>\n",
       "      <td>-1034.48</td>\n",
       "      <td>-998.34</td>\n",
       "      <td>-1022.71</td>\n",
       "      <td>-989.57</td>\n",
       "      <td>-970.88</td>\n",
       "      <td>...</td>\n",
       "      <td>-521.95</td>\n",
       "      <td>-594.37</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-357.24</td>\n",
       "      <td>-443.76</td>\n",
       "      <td>-438.54</td>\n",
       "      <td>-399.71</td>\n",
       "      <td>-384.65</td>\n",
       "      <td>-411.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6  FLUX.7   FLUX.8  \\\n",
       "0    93.85    83.81    20.10   -26.98   -39.56  -124.71 -135.18   -96.27   \n",
       "1   -38.88   -33.83   -58.54   -40.09   -79.31   -72.81  -86.55   -85.33   \n",
       "2   532.64   535.92   513.73   496.92   456.45   466.00  464.50   486.39   \n",
       "3   326.52   347.39   302.35   298.13   317.74   312.70  322.33   311.31   \n",
       "4 -1107.21 -1112.59 -1118.95 -1095.10 -1057.55 -1034.48 -998.34 -1022.71   \n",
       "\n",
       "   FLUX.9  FLUX.10  ...  FLUX.3187  FLUX.3188  FLUX.3189  FLUX.3190  \\\n",
       "0  -79.89  -160.17  ...     -16.51     -78.07    -102.15    -102.15   \n",
       "1  -83.97   -73.38  ...      12.53      -3.28     -32.21     -32.21   \n",
       "2  436.56   484.39  ...     -83.83     -71.69      13.31      13.31   \n",
       "3  312.42   323.33  ...       7.42       5.71      -3.73      -3.73   \n",
       "4 -989.57  -970.88  ...    -521.95    -594.37    -401.66    -401.66   \n",
       "\n",
       "   FLUX.3191  FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  \n",
       "0      25.13      48.57      92.54      39.32      61.42       5.08  \n",
       "1     -24.89      -4.86       0.76     -11.70       6.46      16.00  \n",
       "2     -29.89     -20.88       5.06     -11.80     -28.91     -70.02  \n",
       "3      30.05      20.03     -12.67      -8.77     -17.31     -17.35  \n",
       "4    -357.24    -443.76    -438.54    -399.71    -384.65    -411.79  \n",
       "\n",
       "[5 rows x 3196 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL\n",
       "0      2\n",
       "1      2\n",
       "2      2\n",
       "3      2\n",
       "4      2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91629\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "C:\\Users\\91629\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('exoTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "      <th>FLUX.3197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>119.88</td>\n",
       "      <td>100.21</td>\n",
       "      <td>86.46</td>\n",
       "      <td>48.68</td>\n",
       "      <td>46.12</td>\n",
       "      <td>39.39</td>\n",
       "      <td>18.57</td>\n",
       "      <td>6.98</td>\n",
       "      <td>6.63</td>\n",
       "      <td>...</td>\n",
       "      <td>14.52</td>\n",
       "      <td>19.29</td>\n",
       "      <td>14.44</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>13.33</td>\n",
       "      <td>45.50</td>\n",
       "      <td>31.93</td>\n",
       "      <td>35.78</td>\n",
       "      <td>269.43</td>\n",
       "      <td>57.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5736.59</td>\n",
       "      <td>5699.98</td>\n",
       "      <td>5717.16</td>\n",
       "      <td>5692.73</td>\n",
       "      <td>5663.83</td>\n",
       "      <td>5631.16</td>\n",
       "      <td>5626.39</td>\n",
       "      <td>5569.47</td>\n",
       "      <td>5550.44</td>\n",
       "      <td>...</td>\n",
       "      <td>-581.91</td>\n",
       "      <td>-984.09</td>\n",
       "      <td>-1230.89</td>\n",
       "      <td>-1600.45</td>\n",
       "      <td>-1824.53</td>\n",
       "      <td>-2061.17</td>\n",
       "      <td>-2265.98</td>\n",
       "      <td>-2366.19</td>\n",
       "      <td>-2294.86</td>\n",
       "      <td>-2034.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>844.48</td>\n",
       "      <td>817.49</td>\n",
       "      <td>770.07</td>\n",
       "      <td>675.01</td>\n",
       "      <td>605.52</td>\n",
       "      <td>499.45</td>\n",
       "      <td>440.77</td>\n",
       "      <td>362.95</td>\n",
       "      <td>207.27</td>\n",
       "      <td>...</td>\n",
       "      <td>17.82</td>\n",
       "      <td>-51.66</td>\n",
       "      <td>-48.29</td>\n",
       "      <td>-59.99</td>\n",
       "      <td>-82.10</td>\n",
       "      <td>-174.54</td>\n",
       "      <td>-95.23</td>\n",
       "      <td>-162.68</td>\n",
       "      <td>-36.79</td>\n",
       "      <td>30.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>-826.00</td>\n",
       "      <td>-827.31</td>\n",
       "      <td>-846.12</td>\n",
       "      <td>-836.03</td>\n",
       "      <td>-745.50</td>\n",
       "      <td>-784.69</td>\n",
       "      <td>-791.22</td>\n",
       "      <td>-746.50</td>\n",
       "      <td>-709.53</td>\n",
       "      <td>...</td>\n",
       "      <td>122.34</td>\n",
       "      <td>93.03</td>\n",
       "      <td>93.03</td>\n",
       "      <td>68.81</td>\n",
       "      <td>9.81</td>\n",
       "      <td>20.75</td>\n",
       "      <td>20.25</td>\n",
       "      <td>-120.81</td>\n",
       "      <td>-257.56</td>\n",
       "      <td>-215.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-39.57</td>\n",
       "      <td>-15.88</td>\n",
       "      <td>-9.16</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>-16.13</td>\n",
       "      <td>-24.05</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-45.20</td>\n",
       "      <td>-5.04</td>\n",
       "      <td>...</td>\n",
       "      <td>-37.87</td>\n",
       "      <td>-61.85</td>\n",
       "      <td>-27.15</td>\n",
       "      <td>-21.18</td>\n",
       "      <td>-33.76</td>\n",
       "      <td>-85.34</td>\n",
       "      <td>-81.46</td>\n",
       "      <td>-61.98</td>\n",
       "      <td>-69.34</td>\n",
       "      <td>-17.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL   FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6   FLUX.7  \\\n",
       "0      2   119.88   100.21    86.46    48.68    46.12    39.39    18.57   \n",
       "1      2  5736.59  5699.98  5717.16  5692.73  5663.83  5631.16  5626.39   \n",
       "2      2   844.48   817.49   770.07   675.01   605.52   499.45   440.77   \n",
       "3      2  -826.00  -827.31  -846.12  -836.03  -745.50  -784.69  -791.22   \n",
       "4      2   -39.57   -15.88    -9.16    -6.37   -16.13   -24.05    -0.90   \n",
       "\n",
       "    FLUX.8   FLUX.9  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
       "0     6.98     6.63  ...      14.52      19.29      14.44      -1.62   \n",
       "1  5569.47  5550.44  ...    -581.91    -984.09   -1230.89   -1600.45   \n",
       "2   362.95   207.27  ...      17.82     -51.66     -48.29     -59.99   \n",
       "3  -746.50  -709.53  ...     122.34      93.03      93.03      68.81   \n",
       "4   -45.20    -5.04  ...     -37.87     -61.85     -27.15     -21.18   \n",
       "\n",
       "   FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
       "0      13.33      45.50      31.93      35.78     269.43      57.72  \n",
       "1   -1824.53   -2061.17   -2265.98   -2366.19   -2294.86   -2034.72  \n",
       "2     -82.10    -174.54     -95.23    -162.68     -36.79      30.63  \n",
       "3       9.81      20.75      20.25    -120.81    -257.56    -215.41  \n",
       "4     -33.76     -85.34     -81.46     -61.98     -69.34     -17.84  \n",
       "\n",
       "[5 rows x 3198 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 3198)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=test_data[data.columns[1:3197]]\n",
    "y_test=test_data[['LABEL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5403508771929825"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                102304    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 113,921\n",
      "Trainable params: 113,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2= Sequential()\n",
    "model2.add(Dense(32,input_shape=(x_train.shape[1],)))\n",
    "model2.add(Dense(32,Activation('relu')))\n",
    "model2.add(Dense(64,Activation('relu')))\n",
    "model2.add(Dense(128,Activation('relu')))\n",
    "model2.add(Dense(1))\n",
    "learning_rate=0.01\n",
    "optimizer=optimizers.Adam(learning_rate)\n",
    "model2.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "317/317 [==============================] - 0s 953us/step - loss: -24958536.0000 - accuracy: 0.9923\n",
      "Epoch 2/100\n",
      "317/317 [==============================] - 0s 868us/step - loss: -27683493888.0000 - accuracy: 0.9927\n",
      "Epoch 3/100\n",
      "317/317 [==============================] - 0s 941us/step - loss: -201136914432.0000 - accuracy: 0.9927\n",
      "Epoch 4/100\n",
      "317/317 [==============================] - 0s 875us/step - loss: -146076237824.0000 - accuracy: 0.9929\n",
      "Epoch 5/100\n",
      "317/317 [==============================] - 0s 881us/step - loss: -2524462710784.0000 - accuracy: 0.9925\n",
      "Epoch 6/100\n",
      "317/317 [==============================] - 0s 881us/step - loss: -3988918894592.0000 - accuracy: 0.9927\n",
      "Epoch 7/100\n",
      "317/317 [==============================] - 0s 966us/step - loss: -7402833838080.0000 - accuracy: 0.9927\n",
      "Epoch 8/100\n",
      "317/317 [==============================] - 0s 894us/step - loss: -12673162936320.0000 - accuracy: 0.9931\n",
      "Epoch 9/100\n",
      "317/317 [==============================] - 0s 887us/step - loss: -20302134247424.0000 - accuracy: 0.9925\n",
      "Epoch 10/100\n",
      "317/317 [==============================] - 0s 931us/step - loss: -32010162667520.0000 - accuracy: 0.9929\n",
      "Epoch 11/100\n",
      "317/317 [==============================] - 0s 938us/step - loss: -48382169055232.0000 - accuracy: 0.9923\n",
      "Epoch 12/100\n",
      "317/317 [==============================] - 0s 878us/step - loss: -68386214117376.0000 - accuracy: 0.9929\n",
      "Epoch 13/100\n",
      "317/317 [==============================] - 0s 900us/step - loss: -94917560893440.0000 - accuracy: 0.9929\n",
      "Epoch 14/100\n",
      "317/317 [==============================] - 0s 941us/step - loss: -130099240239104.0000 - accuracy: 0.9925\n",
      "Epoch 15/100\n",
      "317/317 [==============================] - 0s 922us/step - loss: -170314663198720.0000 - accuracy: 0.9925\n",
      "Epoch 16/100\n",
      "317/317 [==============================] - 0s 890us/step - loss: -219900714942464.0000 - accuracy: 0.9929\n",
      "Epoch 17/100\n",
      "317/317 [==============================] - 0s 934us/step - loss: -268208175054848.0000 - accuracy: 0.9927\n",
      "Epoch 18/100\n",
      "317/317 [==============================] - 0s 950us/step - loss: -359456516866048.0000 - accuracy: 0.9927\n",
      "Epoch 19/100\n",
      "317/317 [==============================] - 0s 890us/step - loss: -431126468362240.0000 - accuracy: 0.9929\n",
      "Epoch 20/100\n",
      "317/317 [==============================] - 0s 900us/step - loss: -525646350516224.0000 - accuracy: 0.9925\n",
      "Epoch 21/100\n",
      "317/317 [==============================] - 0s 916us/step - loss: -134917312741376.0000 - accuracy: 0.9933\n",
      "Epoch 22/100\n",
      "317/317 [==============================] - 0s 960us/step - loss: -1308932890951680.0000 - accuracy: 0.9919\n",
      "Epoch 23/100\n",
      "317/317 [==============================] - 0s 894us/step - loss: -923930814578688.0000 - accuracy: 0.9931\n",
      "Epoch 24/100\n",
      "317/317 [==============================] - 0s 900us/step - loss: -1092876373065728.0000 - accuracy: 0.9923\n",
      "Epoch 25/100\n",
      "317/317 [==============================] - 0s 953us/step - loss: -1257763724328960.0000 - accuracy: 0.9931\n",
      "Epoch 26/100\n",
      "317/317 [==============================] - 0s 912us/step - loss: -314301378199552.0000 - accuracy: 0.9927\n",
      "Epoch 27/100\n",
      "317/317 [==============================] - 0s 906us/step - loss: -2952367498592256.0000 - accuracy: 0.9927\n",
      "Epoch 28/100\n",
      "317/317 [==============================] - 0s 859us/step - loss: -2014584340742144.0000 - accuracy: 0.9929\n",
      "Epoch 29/100\n",
      "317/317 [==============================] - 0s 934us/step - loss: -2212130824650752.0000 - accuracy: 0.9931\n",
      "Epoch 30/100\n",
      "317/317 [==============================] - 0s 843us/step - loss: -2700698789609472.0000 - accuracy: 0.9923\n",
      "Epoch 31/100\n",
      "317/317 [==============================] - 0s 944us/step - loss: -3007247785394176.0000 - accuracy: 0.9929\n",
      "Epoch 32/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -3497253356437504.0000 - accuracy: 0.9921\n",
      "Epoch 33/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -650621845766144.0000 - accuracy: 0.9933\n",
      "Epoch 34/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -7542483873431552.0000 - accuracy: 0.9923\n",
      "Epoch 35/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -4936640778731520.0000 - accuracy: 0.9931\n",
      "Epoch 36/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -5312096619200512.0000 - accuracy: 0.9927\n",
      "Epoch 37/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -6210029325647872.0000 - accuracy: 0.9927\n",
      "Epoch 38/100\n",
      "317/317 [==============================] - 0s 2ms/step - loss: -1129568077348864.0000 - accuracy: 0.9925\n",
      "Epoch 39/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -12237888687177728.0000 - accuracy: 0.9947\n",
      "Epoch 40/100\n",
      "317/317 [==============================] - 0s 925us/step - loss: -8795697348345856.0000 - accuracy: 0.9907\n",
      "Epoch 41/100\n",
      "317/317 [==============================] - 0s 916us/step - loss: -9171609529090048.0000 - accuracy: 0.9927\n",
      "Epoch 42/100\n",
      "317/317 [==============================] - 1s 2ms/step - loss: -1661218993471488.0000 - accuracy: 0.9937\n",
      "Epoch 43/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -19055687560593408.0000 - accuracy: 0.9923\n",
      "Epoch 44/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -12414219643256832.0000 - accuracy: 0.9919\n",
      "Epoch 45/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -13055697704976384.0000 - accuracy: 0.9935\n",
      "Epoch 46/100\n",
      "317/317 [==============================] - 0s 2ms/step - loss: -14468250171604992.0000 - accuracy: 0.9929\n",
      "Epoch 47/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -15431266664972288.0000 - accuracy: 0.9925\n",
      "Epoch 48/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -17124530677874688.0000 - accuracy: 0.9923\n",
      "Epoch 49/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -19018972032663552.0000 - accuracy: 0.9929\n",
      "Epoch 50/100\n",
      "317/317 [==============================] - 0s 928us/step - loss: -19623690543038464.0000 - accuracy: 0.9933\n",
      "Epoch 51/100\n",
      "317/317 [==============================] - 0s 906us/step - loss: -21199148971720704.0000 - accuracy: 0.9925\n",
      "Epoch 52/100\n",
      "317/317 [==============================] - 0s 912us/step - loss: -24669362287804416.0000 - accuracy: 0.9921\n",
      "Epoch 53/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -24189217156366336.0000 - accuracy: 0.9937\n",
      "Epoch 54/100\n",
      "317/317 [==============================] - 0s 900us/step - loss: -28482032673751040.0000 - accuracy: 0.9919\n",
      "Epoch 55/100\n",
      "317/317 [==============================] - 0s 865us/step - loss: -29449053150380032.0000 - accuracy: 0.9925\n",
      "Epoch 56/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -33027205437063168.0000 - accuracy: 0.9929\n",
      "Epoch 57/100\n",
      "317/317 [==============================] - 0s 978us/step - loss: -32523244711968768.0000 - accuracy: 0.9941\n",
      "Epoch 58/100\n",
      "317/317 [==============================] - 0s 865us/step - loss: -36233757153296384.0000 - accuracy: 0.9921\n",
      "Epoch 59/100\n",
      "317/317 [==============================] - 0s 868us/step - loss: -41504343450451968.0000 - accuracy: 0.9915\n",
      "Epoch 60/100\n",
      "317/317 [==============================] - 0s 982us/step - loss: -42042034701205504.0000 - accuracy: 0.9933\n",
      "Epoch 61/100\n",
      "317/317 [==============================] - 0s 925us/step - loss: -44183209042247680.0000 - accuracy: 0.9935\n",
      "Epoch 62/100\n",
      "317/317 [==============================] - 0s 909us/step - loss: -48839022310588416.0000 - accuracy: 0.9923\n",
      "Epoch 63/100\n",
      "317/317 [==============================] - 0s 916us/step - loss: -52244274246320128.0000 - accuracy: 0.9921\n",
      "Epoch 64/100\n",
      "317/317 [==============================] - 0s 928us/step - loss: -54202796513165312.0000 - accuracy: 0.9927\n",
      "Epoch 65/100\n",
      "317/317 [==============================] - 0s 865us/step - loss: -58365530356056064.0000 - accuracy: 0.9929\n",
      "Epoch 66/100\n",
      "317/317 [==============================] - 0s 900us/step - loss: -64146281458565120.0000 - accuracy: 0.9925\n",
      "Epoch 67/100\n",
      "317/317 [==============================] - 0s 947us/step - loss: -65260086737502208.0000 - accuracy: 0.9935\n",
      "Epoch 68/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -11298184202551296.0000 - accuracy: 0.9925\n",
      "Epoch 69/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -74558295796350976.0000 - accuracy: 0.9919\n",
      "Epoch 70/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -140135462789447680.0000 - accuracy: 0.9939\n",
      "Epoch 71/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -86648121928450048.0000 - accuracy: 0.9917\n",
      "Epoch 72/100\n",
      "317/317 [==============================] - 0s 941us/step - loss: -17133425555144704.0000 - accuracy: 0.9927\n",
      "Epoch 73/100\n",
      "317/317 [==============================] - 0s 994us/step - loss: -167347731331809280.0000 - accuracy: 0.9923\n",
      "Epoch 74/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -91645239067934720.0000 - accuracy: 0.9949\n",
      "Epoch 75/100\n",
      "317/317 [==============================] - 0s 887us/step - loss: -110390426262831104.0000 - accuracy: 0.9917\n",
      "Epoch 76/100\n",
      "317/317 [==============================] - 0s 859us/step - loss: -116386604334972928.0000 - accuracy: 0.9923\n",
      "Epoch 77/100\n",
      "317/317 [==============================] - 0s 938us/step - loss: -20426280344223744.0000 - accuracy: 0.9927\n",
      "Epoch 78/100\n",
      "317/317 [==============================] - 0s 853us/step - loss: -219218291883769856.0000 - accuracy: 0.9931\n",
      "Epoch 79/100\n",
      "317/317 [==============================] - 0s 853us/step - loss: -22775777778991104.0000 - accuracy: 0.9937\n",
      "Epoch 80/100\n",
      "317/317 [==============================] - 0s 862us/step - loss: -252757622778757120.0000 - accuracy: 0.9919\n",
      "Epoch 81/100\n",
      "317/317 [==============================] - 0s 916us/step - loss: -149389854591942656.0000 - accuracy: 0.9923\n",
      "Epoch 82/100\n",
      "317/317 [==============================] - 0s 856us/step - loss: -151761535532793856.0000 - accuracy: 0.9923\n",
      "Epoch 83/100\n",
      "317/317 [==============================] - 0s 868us/step - loss: -161638001808506880.0000 - accuracy: 0.9931\n",
      "Epoch 84/100\n",
      "317/317 [==============================] - 0s 871us/step - loss: -33247404115361792.0000 - accuracy: 0.9931\n",
      "Epoch 85/100\n",
      "317/317 [==============================] - 0s 919us/step - loss: -312995466818093056.0000 - accuracy: 0.9925\n",
      "Epoch 86/100\n",
      "317/317 [==============================] - 0s 862us/step - loss: -190094427887239168.0000 - accuracy: 0.9931\n",
      "Epoch 87/100\n",
      "317/317 [==============================] - 0s 862us/step - loss: -195680960568623104.0000 - accuracy: 0.9923\n",
      "Epoch 88/100\n",
      "317/317 [==============================] - 0s 868us/step - loss: -48317527381508096.0000 - accuracy: 0.9927\n",
      "Epoch 89/100\n",
      "317/317 [==============================] - 0s 938us/step - loss: -381029088816529408.0000 - accuracy: 0.9935\n",
      "Epoch 90/100\n",
      "317/317 [==============================] - 0s 878us/step - loss: -45400785026023424.0000 - accuracy: 0.9931\n",
      "Epoch 91/100\n",
      "317/317 [==============================] - 0s 856us/step - loss: -417549608051277824.0000 - accuracy: 0.9921\n",
      "Epoch 92/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -46790486119153664.0000 - accuracy: 0.9927\n",
      "Epoch 93/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -247961157921406976.0000 - accuracy: 0.9927\n",
      "Epoch 94/100\n",
      "317/317 [==============================] - 0s 1ms/step - loss: -500262397475291136.0000 - accuracy: 0.9921\n",
      "Epoch 95/100\n",
      "317/317 [==============================] - 0s 912us/step - loss: -278268955422883840.0000 - accuracy: 0.9929\n",
      "Epoch 96/100\n",
      "317/317 [==============================] - 0s 894us/step - loss: -295788865757642752.0000 - accuracy: 0.9935\n",
      "Epoch 97/100\n",
      "317/317 [==============================] - 0s 868us/step - loss: -331461111771561984.0000 - accuracy: 0.9917\n",
      "Epoch 98/100\n",
      "317/317 [==============================] - 0s 856us/step - loss: -305416223930187776.0000 - accuracy: 0.9939\n",
      "Epoch 99/100\n",
      "317/317 [==============================] - 0s 890us/step - loss: -338780423238713344.0000 - accuracy: 0.9931\n",
      "Epoch 100/100\n",
      "317/317 [==============================] - 0s 909us/step - loss: -80730653426974720.0000 - accuracy: 0.9901\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "batch_size=16\n",
    "with tf.device('/CPU:0'):\n",
    "    history=model2.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    steps_per_epoch=int(x_train.shape[0]/batch_size),\n",
    "    validation_data=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-2.957889e+17</td>\n",
       "      <td>0.993492</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-3.314611e+17</td>\n",
       "      <td>0.991718</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-3.054162e+17</td>\n",
       "      <td>0.993887</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-3.387804e+17</td>\n",
       "      <td>0.993098</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-8.073065e+16</td>\n",
       "      <td>0.990140</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  accuracy  epoch\n",
       "95 -2.957889e+17  0.993492     95\n",
       "96 -3.314611e+17  0.991718     96\n",
       "97 -3.054162e+17  0.993887     97\n",
       "98 -3.387804e+17  0.993098     98\n",
       "99 -8.073065e+16  0.990140     99"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Summary of results')\n",
    "hist=pd.DataFrame(history.history)\n",
    "hist['epoch']=history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: -42463538496667648.0000 - accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-4.246353849666765e+16, 0.9912280440330505]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(x_test,y_test,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "from numpy import asarray\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 3196, 8)           320       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3196, 8)           0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3196, 32)          288       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3196, 1)           33        \n",
      "=================================================================\n",
      "Total params: 641\n",
      "Trainable params: 641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3=Sequential()\n",
    "\n",
    "model3.add(LSTM(8,input_shape=(x_train.shape[1],1), activation='relu',return_sequences=True))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(32,activation='relu'))\n",
    "model3.add(Dense(1,activation='softmax'))\n",
    "learning_rate=1e-3\n",
    "decay_rate=1e-5\n",
    "optimizer=optimizers.Adam(lr=learning_rate,decay=decay_rate)\n",
    "model3.compile(loss='mse',\n",
    "             optimizer=optimizer,\n",
    "             metrics=['mae'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 3196, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=np.array(x_train)\n",
    "x_test=np.array(x_test)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "X_train=x_train.reshape((x_train.shape[0],x_train.shape[1],1))\n",
    "X_test=x_test.reshape((x_test.shape[0],x_test.shape[1],1))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 - 50s - loss: 0.0073 - mae: 0.0073\n",
      "Epoch 2/10\n",
      "80/80 - 50s - loss: 0.0073 - mae: 0.0073\n",
      "Epoch 3/10\n",
      "80/80 - 49s - loss: 0.0073 - mae: 0.0073\n",
      "Epoch 4/10\n",
      "80/80 - 49s - loss: 0.0073 - mae: 0.0073\n",
      "Epoch 5/10\n",
      "80/80 - 50s - loss: 0.0073 - mae: 0.0073\n",
      "Epoch 6/10\n",
      "80/80 - 53s - loss: 0.0073 - mae: 0.0073\n",
      "Epoch 7/10\n",
      "80/80 - 52s - loss: 0.0073 - mae: 0.0073\n",
      "Epoch 8/10\n",
      "80/80 - 53s - loss: 0.0073 - mae: 0.0073\n",
      "Epoch 9/10\n",
      "80/80 - 53s - loss: 0.0073 - mae: 0.0073\n",
      "Epoch 10/10\n",
      "80/80 - 52s - loss: 0.0073 - mae: 0.0073\n"
     ]
    }
   ],
   "source": [
    "history=model3.fit(X_train,y_train,epochs=10,batch_size=64,verbose=2,validation_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 145ms/step - loss: 0.0088 - mae: 0.0088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.008771929889917374, 0.008771929889917374]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test,y_test,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
